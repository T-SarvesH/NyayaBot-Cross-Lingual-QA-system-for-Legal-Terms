""" Preprocessing step 1: Tokenization and the docs used are Constitution of India and Maharashtra State Laws """

from nltk.tokenize import word_tokenize, sent_tokenize
import spacy
import nltk
import re
from typing import List, Dict, Set, Tuple
import fitz, pymupdf

nltk.download('punkt', quiet=True)

def word_tokenizer(input: List[str]):
    pass

def sentence_tokenizer(input: List[str]):
    pass

def tokenize_main():
    pass
    
if __name__ == "__main__":
    tokenize_main()